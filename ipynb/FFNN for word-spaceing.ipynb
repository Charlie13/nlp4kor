{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이 문서는 비전공 초보가 제작하였으므로, 틀린 부분이 있을지도 모릅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 설치 및 설정은 아래 페이지를 참고하여 주세요.\n",
    "- https://github.com/bage79/nlp4kor/blob/master/INSTALL.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 다양한 유틸리티 클래스가 배포되었습니다.\n",
    "- https://github.com/bage79/nlp4kor/tree/master/bage_utils\n",
    "- 차후 강좌에서 계속 사용될 예정이오니, 아래에 import 되는 클래스들은 미리 한번 보시면 이해하시는데, 큰 도움이 되실 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFNN for 한글 띄어쓰기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/word_spacing.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 소스 코드: https://github.com/hunkim/DeepLearningZeroToAll/blob/master/lab-09-2-xor-nn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한글 문자(음절) 데이터를 입력하기 위해 변환하려면? (text -> vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (방법1) 1d-array one-hot-vector (predefined dictionary)\n",
    "- 자음, 모음, 완성형, 영어/숫자/특수문자 의 모든 문자를 one hot vector 로 표시\n",
    "- vector 종류 수 = 11,316\n",
    "- 미리 vector 변환을 생성해 둘 수 있으나, vector의 크기가 매우 큼. 또한 한자등 다른 문자들은 제외됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bage_utils.hangul_util import HangulUtil # 한글처리\n",
    "from bage_utils.num_util import NumUtil # 숫자(int, str) 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "자음: len: 30 ('ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ')\n",
      "모음: len: 21 ('ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ')\n",
      "완성형: len: 11172 ('가', '각', '갂', '갃', '간', '갅', '갆', '갇', '갈', '갉') ... ('힚', '힛', '힜', '힝', '힞', '힟', '힠', '힡', '힢', '힣')\n",
      "전체 한글(자음+모음+완성형) len: 11,223\n"
     ]
    }
   ],
   "source": [
    "print('자음:', 'len:', len(HangulUtil.JA_LIST), HangulUtil.JA_LIST)\n",
    "print('모음:', 'len:', len(HangulUtil.MO_LIST), HangulUtil.MO_LIST)\n",
    "print('완성형:', 'len:', len(HangulUtil.WANSUNG_LIST), \n",
    "      HangulUtil.WANSUNG_LIST[:10], '...', HangulUtil.WANSUNG_LIST[-10:])\n",
    "print('전체 한글(자음+모음+완성형)', 'len:', NumUtil.comma_str(len(HangulUtil.HANGUL_LIST)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 한글(자음+모음+완성형) + 키보드특수문자: len: 11,317\n",
      "len(one-hot-vector): 11,317\n",
      "ㄱ 0 [1 0 0 ..., 0 0 0]\n",
      "ㅏ 30 [0 0 0 ..., 0 0 0]\n",
      "가 51 [0 0 0 ..., 0 0 0]\n",
      "힣 11222 [0 0 0 ..., 0 0 0]\n",
      "A 11249 [0 0 0 ..., 0 0 0]\n",
      "a 11223 [0 0 0 ..., 0 0 0]\n",
      "0 11275 [0 0 0 ..., 0 0 0]\n",
      "? 11316 [0 0 0 ..., 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print('전체 한글(자음+모음+완성형) + 키보드특수문자:', 'len:', NumUtil.comma_str(len(HangulUtil.CHAR_LIST)))\n",
    "print('len(one-hot-vector):', NumUtil.comma_str(len(HangulUtil.to_one_hot_vector('ㄱ'))))\n",
    "print('ㄱ', HangulUtil.to_one_hot_index('ㄱ'), HangulUtil.to_one_hot_vector('ㄱ'))\n",
    "print('ㅏ', HangulUtil.to_one_hot_index('ㅏ'), HangulUtil.to_one_hot_vector('ㅏ'))\n",
    "print('가', HangulUtil.to_one_hot_index('가'), HangulUtil.to_one_hot_vector('가'))\n",
    "print('힣', HangulUtil.to_one_hot_index('힣'), HangulUtil.to_one_hot_vector('힣'))\n",
    "print('A', HangulUtil.to_one_hot_index('A'), HangulUtil.to_one_hot_vector('A'))\n",
    "print('a', HangulUtil.to_one_hot_index('a'), HangulUtil.to_one_hot_vector('a'))\n",
    "print('0', HangulUtil.to_one_hot_index('0'), HangulUtil.to_one_hot_vector('0'))\n",
    "print('?', HangulUtil.to_one_hot_index('?'), HangulUtil.to_one_hot_vector('?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (방법2) 1d-array non-one-hot-vector\n",
    "- 초성, 중성, 종성, 기타 순서로 3개의 vector를 생성한 후, 1d-array로 concate.\n",
    "- 한글이 아닌 경우에는 중성, 종성 부분은 항상 0.\n",
    "- 경우의 수 = (초성개수*중성개수*종성개수) + 영어개수 + 숫자개수 + 특수문자개수 + ....\n",
    "- 문자 종류별로 범위를 정의해 주어야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초성: len: 19 ('ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ')\n",
      "중성: len: 21 ('ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ')\n",
      "종성: len: 28 ('', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ')\n",
      "영어: len: 52 ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
      "숫자: len: 10 ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "특수문자: len: 32 ['`', '~', '!', '@', '#', '$', '%', '^', '&', '*', '(', ')', '_', '+', '-', '+', '[', ']', '\\\\', ';', \"'\", ',', '.', '/', '{', '}', '|', ':', '\"', '<', '>', '?']\n"
     ]
    }
   ],
   "source": [
    "print('초성:', 'len:', len(HangulUtil.CHO_LIST), HangulUtil.CHO_LIST)\n",
    "print('중성:', 'len:', len(HangulUtil.JUNG_LIST), HangulUtil.JUNG_LIST)\n",
    "print('종성:', 'len:', len(HangulUtil.JONG_LIST), HangulUtil.JONG_LIST)\n",
    "print('영어:', 'len:', len(HangulUtil.ENGLISH_LIST), HangulUtil.ENGLISH_LIST)\n",
    "print('숫자:', 'len:', len(HangulUtil.NUM_LIST), HangulUtil.NUM_LIST)\n",
    "print('특수문자:', 'len:', len(HangulUtil.KEYBOARD_SPECIAL_LIST), HangulUtil.KEYBOARD_SPECIAL_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len: 162\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "print('len:', len(HangulUtil.to_cho_jung_jong_vector('?')))\n",
    "print(HangulUtil.to_cho_jung_jong_vector('ㄱ'))\n",
    "print(HangulUtil.to_cho_jung_jong_vector('a'))\n",
    "print(HangulUtil.to_cho_jung_jong_vector('1'))\n",
    "print(HangulUtil.to_cho_jung_jong_vector('각'))\n",
    "print(HangulUtil.to_cho_jung_jong_vector('?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (방법3)  1d-array one-hot-vector (from corpus)\n",
    "- 한자 및 다른 문자들도 처리하고 싶고, 한글의 모든 문자를 항상 사용하는 것이 아님.\n",
    "- 따라서, 적당히 큰 말뭉치(corpus)에서 실제 사용되는 음절을 추출하여 사용.\n",
    "- 이 발표자료의 FFNN에서는 이 방법으로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습용 말뭉치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 bage bage 204M  6월  2 11:58 /home/bage/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.sentences.gz\n",
      "7601655\n"
     ]
    }
   ],
   "source": [
    "# 총 문장 수 (dump corpus (ko.wikipedia.org) 문장 단위로 분리. 총 문장: 7,601,655 / 총 문서: 518,062)\n",
    "!ls -lh ~/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.sentences.gz\n",
    "!zcat ~/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.sentences.gz | wc -l # Ubuntu\n",
    "# !gzcat ~/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.sentences.gz | wc -l # OSX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!는 문장부호의 하나로서 겹느낌표라고 부른다.\r\n",
      "!!는 체스에서 '훌륭한 이동'을 했을 때를 나타낸다.\r\n",
      "!!는 수학에서 이중 팩토리얼을 나타낸다.\r\n",
      "!!! (칙칙칙으로 읽음)은 1996년 결성된 미국의 댄스 펑크 밴드이다.\r\n",
      "\"Me and Giuliani Down by the Schoolyard (a true story)\"나 \"Must Be the Moon\"로 알려졌다.\r\n",
      "새크라멘토 출신이지만 지금은 뉴욕 시, 새크라멘토, 포틀랜드에 기반을 두고 있다.\r\n",
      "2006년 레드 핫 칠리 페퍼스의 존 프루시안테가 LA에서 이들의 공연을 보고 영국 순회 공연에 이들을 서포트 밴드로 초대하기도 했다.\r\n",
      "《! -attention-》은 1998년 9월 30일 발매한 20th Century의 2번째 앨범이다.\r\n",
      "전 곡이 어두운 곡으로 선택되었고 초회한정반, 통상반으로 2가지 형태로 발매되었다.\r\n",
      "초회한정반 특전으로 20th Century 오리지날 키홀더를 봉입했다.\r\n",
      "\r\n",
      "gzip: stdout: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!zcat ~/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.sentences.gz | head # Ubuntu (Broken pipe error on only Jupyter)\n",
    "# !gzcat -cd ~/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.sentences.gz | head # OSX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 bage bage 67K  6월  2 11:58 /home/bage/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.characters\n",
      "17382\n",
      "\n",
      "\n",
      " \n",
      "!\n",
      "\"\n",
      "#\n",
      "$\n",
      "%\n",
      "&\n",
      "'\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ~/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.characters\n",
    "!cat ~/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.characters | wc -l \n",
    "!cat ~/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.characters | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한\n",
      "뷁\n"
     ]
    }
   ],
   "source": [
    "!grep --color=never 한 ~/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.characters\n",
    "!grep --color=never 뷁 ~/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.characters\n",
    "!grep --color=never 쀓 ~/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.characters # not found in corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## character (from corpus) -> Vector \n",
    "x_data -> y_data => 로 표시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/FFNN for word-spacing.001.jpeg\">\n",
    "<img src=\"./img/FFNN for word-spacing.002.jpeg\">\n",
    "<img src=\"./img/FFNN for word-spacing.003.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전처리 (text_preprocessing.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from bage_utils.base_util import is_my_pc\n",
    "from bage_utils.datafile_util import DataFileUtil\n",
    "from bage_utils.dataset import DataSet\n",
    "from bage_utils.file_util import FileUtil\n",
    "from bage_utils.hangul_util import HangulUtil\n",
    "from bage_utils.mongodb_util import MongodbUtil\n",
    "from bage_utils.num_util import NumUtil\n",
    "from bage_utils.one_hot_vector import OneHotVector\n",
    "from nlp4kor.config import log, KO_WIKIPEDIA_ORG_SENTENCES_FILE, KO_WIKIPEDIA_ORG_DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 입력데이터(문장) 파일 생성 (Mongodb -> file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences_file: /home/bage/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.sentences.gz\n"
     ]
    }
   ],
   "source": [
    "sentences_file = KO_WIKIPEDIA_ORG_SENTENCES_FILE\n",
    "log.info('sentences_file: %s' % sentences_file)\n",
    "if not os.path.exists(sentences_file):\n",
    "    TextPreprocess.dump_corpus(MONGO_URL, db_name='parsed', collection_name='ko.wikipedia.org', sentences_file=sentences_file,\n",
    "                               mongo_query={})  # mongodb -> text file(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 벡터매핑사전(음절) 파일 생성 (문장-> 음절)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "characters_file: /home/bage/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.characters\n"
     ]
    }
   ],
   "source": [
    "characters_file = os.path.join(KO_WIKIPEDIA_ORG_DATA_DIR, 'ko.wikipedia.org.characters')\n",
    "log.info('characters_file: %s' % characters_file)\n",
    "if not os.path.exists(characters_file):\n",
    "    log.info('collect characters...')\n",
    "    TextPreprocess.collect_characters(sentences_file, characters_file)  # text file -> characters(unique features)\n",
    "    log.info('collect characters OK.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      \t unary\tbinary\tternary\n",
      "     0\t   [0]\t   [0]\t[1 0 0]\n",
      "     1\t   [0]\t   [1]\t[0 1 0]\n",
      "     2\t   [0]\t   [0]\t[0 0 1]\n"
     ]
    }
   ],
   "source": [
    "unary_vector = OneHotVector([0])\n",
    "binary_vector = OneHotVector([0, 1])\n",
    "ternary_vector = OneHotVector([0, 1, 2])\n",
    "print('%6s\\t%6s\\t%6s\\t%6s' % ('', 'unary', 'binary', 'ternary'))\n",
    "for i in [0, 1, 2]:\n",
    "    print('%6s\\t%6s\\t%6s\\t%6s' % (i, unary_vector.to_vector(i), binary_vector.to_vector(i), ternary_vector.to_vector(i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot vector for 음절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load characters list...\n",
      "load characters list OK. len: 17,380\n",
      "<bage_utils.one_hot_vector.OneHotVector object at 0x7f0903b872e8> 17380\n",
      "<bage_utils.one_hot_vector.OneHotVector object at 0x7f0903b7f470> 2\n"
     ]
    }
   ],
   "source": [
    "log.info('load characters list...')\n",
    "features_vector = OneHotVector(DataFileUtil.read_list(characters_file))\n",
    "labels_vector = OneHotVector([0, 1])  # 붙여쓰기=0, 띄어쓰기=1\n",
    "log.info('load characters list OK. len: %s' % NumUtil.comma_str(len(features_vector))) # 데이터셋 마다 character 구성과 개수는 다름.\n",
    "\n",
    "print(features_vector, len(features_vector))\n",
    "print(labels_vector, len(labels_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a [0 0 0 ..., 0 0 0] a 64\n",
      "b [0 0 0 ..., 0 0 0] b 65\n",
      "ㄱ [0 0 0 ..., 0 0 0] ㄱ 3129\n",
      "ㄲ [0 0 0 ..., 0 0 0] ㄲ 3130\n",
      "ㄳ [0 0 0 ..., 0 0 0] ㄳ 3131\n",
      "ㄴ [0 0 0 ..., 0 0 0] ㄴ 3132\n",
      "가 [0 0 0 ..., 0 0 0] 가 13891\n",
      "각 [0 0 0 ..., 0 0 0] 각 13892\n"
     ]
    }
   ],
   "source": [
    "for c in ['a', 'b', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', '가', '각']:\n",
    "    v = features_vector.to_vector(c) # one hot vector\n",
    "    _c = features_vector.to_value(v) # character for check\n",
    "    i = features_vector.to_index(c) # index number in characters list (0~17380)\n",
    "    print(c, v, _c, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset (training data, test data, validation data)\n",
    "- 너무 커서 공유하기 힘드네요.. 가장 큰 파일이 약 26GB\n",
    "- 직접 생성하시면 됩니다. ^^ (sentences, characters 파일 이용)\n",
    "\n",
    "##### ko.wikipedia.org.dataset.문장수.left_gram.right_gram.종류.gz\n",
    "- train: text, int 형식 (one-hot-vector로 저장하면 파일이 너무 커짐)\n",
    "- test, validation: one-hot-vector 형식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 38G\r\n",
      "-rw-rw-r-- 1 bage bage 2.6G  6월  2 01:21 ko.wikipedia.org.dataset.1000000.left=2.right=2.test.gz\r\n",
      "-rw-rw-r-- 1 bage bage 3.2G  6월  2 01:21 ko.wikipedia.org.dataset.1000000.left=2.right=2.train.gz\r\n",
      "-rw-rw-r-- 1 bage bage 266M  6월  2 01:21 ko.wikipedia.org.dataset.1000000.left=2.right=2.validation.gz\r\n",
      "-rw-rw-r-- 1 bage bage 2.6G  6월  1 11:20 ko.wikipedia.org.dataset.10000.left=2.right=2.test.gz\r\n",
      "-rw-rw-r-- 1 bage bage  31M  6월  1 11:20 ko.wikipedia.org.dataset.10000.left=2.right=2.train.gz\r\n",
      "-rw-rw-r-- 1 bage bage 266M  6월  1 11:20 ko.wikipedia.org.dataset.10000.left=2.right=2.validation.gz\r\n",
      "-rw-rw-r-- 1 bage bage  93M  6월  1 11:01 ko.wikipedia.org.dataset.100.left=2.right=2.test.gz\r\n",
      "-rw-rw-r-- 1 bage bage 415K  6월  1 11:00 ko.wikipedia.org.dataset.100.left=2.right=2.train.gz\r\n",
      "-rw-rw-r-- 1 bage bage  93M  6월  1 11:00 ko.wikipedia.org.dataset.100.left=2.right=2.validation.gz\r\n",
      "-rw-rw-r-- 1 bage bage 2.6G  6월  1 12:43 ko.wikipedia.org.dataset.7601655.left=2.right=2.test.gz\r\n",
      "-rw-rw-r-- 1 bage bage  26G  6월  1 12:42 ko.wikipedia.org.dataset.7601655.left=2.right=2.train.gz\r\n",
      "-rw-rw-r-- 1 bage bage 266M  6월  1 12:42 ko.wikipedia.org.dataset.7601655.left=2.right=2.validation.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ~/workspace/nlp4kor-ko.wikipedia.org/datasets/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorflow model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 20K\r\n",
      "drwxrwxr-x 2 bage bage 4.0K  6월  2 22:00 word_spacing_model.sentences=1000000.layers=4\r\n",
      "drwxrwxr-x 2 bage bage 4.0K  6월  2 21:58 word_spacing_model.sentences=10000.layers=2\r\n",
      "drwxrwxr-x 2 bage bage 4.0K  6월  2 21:58 word_spacing_model.sentences=10000.layers=4\r\n",
      "drwxrwxr-x 2 bage bage 4.0K  6월  2 21:57 word_spacing_model.sentences=100.layers=2\r\n",
      "drwxrwxr-x 2 bage bage 4.0K  6월  3 21:04 word_spacing_model.sentences=100.layers=4\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ~/workspace/nlp4kor-ko.wikipedia.org/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bage/workspace/nlp4kor-ko.wikipedia.org/models/word_spacing_model.sentences=1000000.layers=4:\r\n",
      "total 80M\r\n",
      "-rw-rw-r-- 1 bage bage 253  6월  2 22:00 checkpoint\r\n",
      "-rw-rw-r-- 1 bage bage 80M  6월  2 22:00 model.data-00000-of-00001\r\n",
      "-rw-rw-r-- 1 bage bage 863  6월  2 22:00 model.index\r\n",
      "-rw-rw-r-- 1 bage bage 65K  6월  2 22:00 model.meta\r\n",
      "\r\n",
      "/home/bage/workspace/nlp4kor-ko.wikipedia.org/models/word_spacing_model.sentences=10000.layers=2:\r\n",
      "total 80M\r\n",
      "-rw-rw-r-- 1 bage bage 251  6월  2 17:10 checkpoint\r\n",
      "-rw-rw-r-- 1 bage bage 80M  6월  2 17:10 model.data-00000-of-00001\r\n",
      "-rw-rw-r-- 1 bage bage 508  6월  2 17:10 model.index\r\n",
      "-rw-rw-r-- 1 bage bage 42K  6월  2 17:10 model.meta\r\n",
      "\r\n",
      "/home/bage/workspace/nlp4kor-ko.wikipedia.org/models/word_spacing_model.sentences=10000.layers=4:\r\n",
      "total 80M\r\n",
      "-rw-rw-r-- 1 bage bage 251  6월  2 16:07 checkpoint\r\n",
      "-rw-rw-r-- 1 bage bage 80M  6월  2 16:07 model.data-00000-of-00001\r\n",
      "-rw-rw-r-- 1 bage bage 863  6월  2 16:07 model.index\r\n",
      "-rw-rw-r-- 1 bage bage 65K  6월  2 16:07 model.meta\r\n",
      "\r\n",
      "/home/bage/workspace/nlp4kor-ko.wikipedia.org/models/word_spacing_model.sentences=100.layers=2:\r\n",
      "total 80M\r\n",
      "-rw-rw-r-- 1 bage bage 257  6월  2 21:57 checkpoint\r\n",
      "-rw-rw-r-- 1 bage bage 80M  6월  2 21:57 model.data-00000-of-00001\r\n",
      "-rw-rw-r-- 1 bage bage 863  6월  2 21:57 model.index\r\n",
      "-rw-rw-r-- 1 bage bage 65K  6월  2 21:57 model.meta\r\n",
      "\r\n",
      "/home/bage/workspace/nlp4kor-ko.wikipedia.org/models/word_spacing_model.sentences=100.layers=4:\r\n",
      "total 80M\r\n",
      "-rw-rw-r-- 1 bage bage 257  6월  3 21:04 checkpoint\r\n",
      "-rw-rw-r-- 1 bage bage 80M  6월  3 21:04 model.data-00000-of-00001\r\n",
      "-rw-rw-r-- 1 bage bage 863  6월  3 21:04 model.index\r\n",
      "-rw-rw-r-- 1 bage bage 65K  6월  3 21:04 model.meta\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ~/workspace/nlp4kor-ko.wikipedia.org/models/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tip. 제 노트북에 GPU가 없어요. GPU 있는 PC에서 개발해야 하나요?\n",
    "- Pycharm Deployment & Remote Interpreter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 각종 설정값 (word_spacing.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from bage_utils.base_util import is_my_pc\n",
    "from bage_utils.datafile_util import DataFileUtil\n",
    "from bage_utils.dataset import DataSet\n",
    "from bage_utils.datasets import DataSets\n",
    "from bage_utils.num_util import NumUtil\n",
    "from bage_utils.one_hot_vector import OneHotVector\n",
    "from bage_utils.watch_util import WatchUtil\n",
    "from nlp4kor.config import log, KO_WIKIPEDIA_ORG_DATA_DIR, KO_WIKIPEDIA_ORG_SENTENCES_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sentences: 1000000\n",
      "layers: 4\n",
      "model_file: /home/bage/workspace/nlp4kor-ko.wikipedia.org/models/word_spacing_model.sentences=1000000.layers=4/model\n",
      "sentences_file: /home/bage/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.sentences.gz\n",
      "characters_file: /home/bage/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.characters\n",
      "batch_size: 1000\n",
      "left_gram: 2, right_gram: 2\n",
      "ngram: 4\n",
      "features_vector: <bage_utils.one_hot_vector.OneHotVector object at 0x7f0903a48ba8>\n",
      "labels_vector: <bage_utils.one_hot_vector.OneHotVector object at 0x7f0903b87278>\n",
      "n_features: 69520\n",
      "n_classes: 1\n",
      "n_hidden1: 100\n",
      "learning_rate: 0.01\n"
     ]
    }
   ],
   "source": [
    "if len(sys.argv) == 2:\n",
    "    max_sentences = int(sys.argv[1])\n",
    "else:\n",
    "    max_sentences = int('1,000,000'.replace(',', '')) if is_my_pc() else int('1,000,000'.replace(',', ''))  # run 100 or 1M data (학습: 17시간 소요)\n",
    "# max_sentences = 100 if is_my_pc() else FileUtil.count_lines(sentences_file, gzip_format=True) # run 100 or full data (학습시간: 5일 소요)\n",
    "layers = 4\n",
    "model_file = os.path.join(KO_WIKIPEDIA_ORG_DATA_DIR, 'models',\n",
    "                          'word_spacing_model.sentences=%s.layers=%s/model' % (max_sentences, layers))  # .%s' % max_sentences\n",
    "log.info('max_sentences: %s' % max_sentences)\n",
    "log.info('layers: %s' % layers)\n",
    "log.info('model_file: %s' % model_file)\n",
    "\n",
    "sentences_file = KO_WIKIPEDIA_ORG_SENTENCES_FILE\n",
    "log.info('sentences_file: %s' % sentences_file)\n",
    "\n",
    "characters_file = os.path.join(KO_WIKIPEDIA_ORG_DATA_DIR, 'ko.wikipedia.org.characters')\n",
    "log.info('characters_file: %s' % characters_file)\n",
    "\n",
    "batch_size = 1000  # mini batch size\n",
    "left_gram, right_gram = 2, 2\n",
    "ngram = left_gram + right_gram\n",
    "log.info('batch_size: %s' % batch_size)\n",
    "log.info('left_gram: %s, right_gram: %s' % (left_gram, right_gram))\n",
    "log.info('ngram: %s' % ngram)\n",
    "\n",
    "features_vector = OneHotVector(DataFileUtil.read_list(characters_file))\n",
    "labels_vector = OneHotVector([0, 1])  # 붙여쓰기=0, 띄어쓰기=1\n",
    "n_features = len(features_vector) * ngram  # number of features = 17,380 * 4\n",
    "n_classes = len(labels_vector) if len(labels_vector) >= 3 else 1  # number of classes = 2 but len=1\n",
    "log.info('features_vector: %s' % features_vector)\n",
    "log.info('labels_vector: %s' % labels_vector)\n",
    "log.info('n_features: %s' % n_features)\n",
    "log.info('n_classes: %s' % n_classes)\n",
    "\n",
    "n_hidden1 = 100\n",
    "learning_rate = 0.01  # 0.1 ~ 0.001\n",
    "log.info('n_hidden1: %s' % n_hidden1)\n",
    "log.info('learning_rate: %s' % learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test with samples. (word_spacing.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample testing...\n",
      "['예쁜', '쁜운', '운동', '동화'] -> [0, 1, 0, 0]\n",
      "in : \"예쁜 운동화\"\n",
      "out: \"예쁜 운동화\"\n",
      "['즐거', '거운', '운동', '동화'] -> [0, 0, 1, 0]\n",
      "in : \"즐거운 동화\"\n",
      "out: \"즐거운 동화\"\n",
      "['삼풍', '풍동', '동화', '화재'] -> [0, 0, 1, 0]\n",
      "in : \"삼풍동 화재\"\n",
      "out: \"삼풍동 화재\"\n",
      "sample testing OK.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nlp4kor.ws.word_spacing import WordSpacing\n",
    "log.info('sample testing...')\n",
    "test_set = ['예쁜 운동화', '즐거운 동화', '삼풍동 화재']\n",
    "for s in test_set:\n",
    "    features, labels = WordSpacing.sentence2features_labels(s, left_gram=1, right_gram=1)\n",
    "    log.info('%s -> %s' % (features, labels))\n",
    "    log.info('in : \"%s\"' % s)\n",
    "    log.info('out: \"%s\"' % WordSpacing.spacing(s.replace(' ', ''), labels))\n",
    "log.info('sample testing OK.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 입력데이터 생성, 학습, 평가 (word_spacing.py)\n",
    "- WordSpacing.learning() in word_spacing.py\n",
    "- <u>내용이 너무 길어 실제 소스로 설명합니다.</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(model_file + '.index') or not os.path.exists(model_file + '.meta'):\n",
    "    WordSpacing.learning(sentences_file, batch_size, left_gram, right_gram, model_file, features_vector, labels_vector, n_hidden1=n_hidden1, max_sentences=max_sentences, learning_rate=learning_rate, layers=layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그리고... \n",
    "- 지금까지 평가한 것은 4음절 단위에 대한 결과입니다. 하지만 우리가 원하는 것은 문장을 입력으로 했을 때의 결과죠.\n",
    "- 새로운 문장으로 제대로 띄어쓰기가 되는지 확인해 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chek result...\n",
      "len(sentences): 100\n",
      "\n",
      "build_FFNN(layers=4)\n",
      "INFO:tensorflow:Restoring parameters from /home/bage/workspace/nlp4kor-ko.wikipedia.org/models/word_spacing_model.sentences=1000000.layers=4/model\n",
      "Restoring parameters from /home/bage/workspace/nlp4kor-ko.wikipedia.org/models/word_spacing_model.sentences=1000000.layers=4/model\n",
      "\n",
      "[0] in : \"아버지가 방에 들어 가신다.\"\n",
      "[0] out: \"아버지가 방에 들어가 신다.\" (accuracy: 81.8%, sim: 66.7%=2/3)\n",
      "\n",
      "[1] in : \"가는 말이 고와야 오는 말이 곱다.\"\n",
      "[1] out: \"가는 말이고와야오는 말이 곱다.\" (accuracy: 84.6%, sim: 60.0%=3/5)\n",
      "\n",
      "[2] in : \"!!는 문장부호의 하나로서 겹느낌표라고 부른다.\"\n",
      "[2] out: \"!!는 문장 부호의 하나로서 겹느낌표라고 부른다.\" (accuracy: 95.2%, sim: 100.0%=4/4)\n",
      "\n",
      "[3] in : \"!!는 체스에서 '훌륭한 이동'을 했을 때를 나타낸다.\"\n",
      "[3] out: \"!!는 체스에서 '훌륭한 이동'을 했을 때를 나타낸다.\" (accuracy: 100.0%, sim: 100.0%=6/6)\n",
      "\n",
      "[4] in : \"!!는 수학에서 이중 팩토리얼을 나타낸다.\"\n",
      "[4] out: \"!!는 수학에서 이중 팩토리얼을 나타낸다.\" (accuracy: 100.0%, sim: 100.0%=4/4)\n",
      "\n",
      "[5] in : \"!!! (칙칙칙으로 읽음)은 1996년 결성된 미국의 댄스 펑크 밴드이다.\"\n",
      "[5] out: \"!!! (칙칙칙으로 읽음)은 1996년 결성된 미국의 댄스 펑크 밴드이다.\" (accuracy: 100.0%, sim: 100.0%=8/8)\n",
      "\n",
      "[6] in : \"\"Me and Giuliani Down by the Schoolyard (a true story)\"나 \"Must Be the Moon\"로 알려졌다.\"\n",
      "[6] out: \"\"Meand Giuliani Down by the Schoolyard (atruestory)\"나 \"Must Bethe Moon\"로 알려졌다.\" (accuracy: 94.0%, sim: 71.4%=10/14)\n",
      "\n",
      "[7] in : \"새크라멘토 출신이지만 지금은 뉴욕 시, 새크라멘토, 포틀랜드에 기반을 두고 있다.\"\n",
      "[7] out: \"새크라멘토 출신이지만 지금은 뉴욕 시, 새크라멘토, 포틀랜드에 기반을 두고 있다.\" (accuracy: 100.0%, sim: 100.0%=9/9)\n",
      "\n",
      "[8] in : \"2006년 레드 핫 칠리 페퍼스의 존 프루시안테가 LA에서 이들의 공연을 보고 영국 순회 공연에 이들을 서포트 밴드로 초대하기도 했다.\"\n",
      "[8] out: \"2006년 레드 핫 칠리 페퍼스의 존 프루시안테가 LA에서 이들의 공연을 보고 영국 순회 공연에 이들을 서포트 밴드로 초대하기도 했다.\" (accuracy: 100.0%, sim: 100.0%=18/18)\n",
      "\n",
      "[9] in : \"《! -attention-》은 1998년 9월 30일 발매한 20th Century의 2번째 앨범이다.\"\n",
      "[9] out: \"《! -attention-》은 1998년 9월 30일 발매한 20th Century의 2번째 앨범이다.\" (accuracy: 100.0%, sim: 100.0%=9/9)\n",
      "\n",
      "[10] in : \"전 곡이 어두운 곡으로 선택되었고 초회한정반, 통상반으로 2가지 형태로 발매되었다.\"\n",
      "[10] out: \"전곡이 어두운 곡으로 선택되었고 초회한정반, 통상반으로 2가지 형태로 발매되었다.\" (accuracy: 97.2%, sim: 88.9%=8/9)\n",
      "\n",
      "[11] in : \"초회한정반 특전으로 20th Century 오리지날 키홀더를 봉입했다.\"\n",
      "[11] out: \"초회한정반 특전으로 20th Century 오리지날키홀더를 봉입했다.\" (accuracy: 96.9%, sim: 83.3%=5/6)\n",
      "\n",
      "[12] in : \"또, 동시 발매의 Coming Century VHS 《? -question-》 초회한정반을 구입하여 두 음반에 들어 있는 응모권을 보내면, V6 오리지날 자명종 & 부재중 안내 서비스 (8cm CD로 2장 셋트)를 추첨으로 2000명 당첨 발표하는 특전도 있다.\"\n",
      "[12] out: \"또, 동시 발매의 Coming Century VHS 《?-question-》초회한정반을 구입하여 두 음반에 들어있는 응모권을 보내면, V6 오리지날자명종 & 부재중 안내 서비스 (8cmCD로 2장 셋트)를 추첨으로 2000명당첨 발표하는 특전도 있다.\" (accuracy: 94.7%, sim: 81.2%=26/32)\n",
      "\n",
      "[13] in : \"!쿵어는 !쿵족이 사용하는 언어이다.\"\n",
      "[13] out: \"! 쿵어는 !쿵족 이 사용하는 언어이다.\" (accuracy: 87.5%, sim: 100.0%=3/3)\n",
      "\n",
      "[14] in : \"!쿵족은 나미비아, 보츠와나, 앙골라에 걸친 칼라하리 사막에 퍼져 살고 있는 민족으로, 코이산 어족 흡착어인 !쿵어를 사용한다.\"\n",
      "[14] out: \"!쿵족은 나미비아, 보츠와나, 앙골라에 걸친 칼라하리 사막에 퍼져 살고 있는 민족으로, 코이산어족 흡착어인! 쿵어를 사용한다.\" (accuracy: 94.5%, sim: 86.7%=13/15)\n",
      "\n",
      "[15] in : \"느낌표는 앞니 뒤에 혀를 대며 강하게 발음한다는 뜻이다.\"\n",
      "[15] out: \"느낌표는 앞니 뒤에 혀를 대며 강하게 발음한다는 뜻이다.\" (accuracy: 100.0%, sim: 100.0%=7/7)\n",
      "\n",
      "[16] in : \"주로 채집 생활을 한다.\"\n",
      "[16] out: \"주로 채집생활을 한다.\" (accuracy: 88.9%, sim: 66.7%=2/3)\n",
      "\n",
      "[17] in : \"《\"HAPPY\" Coming Century, 20th Century Forever》는 2000년 8월 9일 발매한 V6의 5번째 정규 앨범이다.\"\n",
      "[17] out: \"《\"HAPPY \"Coming Century, 20th Century Forever》는 2000년 8월 9일 발매한 V6의 5번째 정규 앨범이다.\" (accuracy: 97.0%, sim: 92.3%=12/13)\n",
      "\n",
      "[18] in : \"검정 CD케이스와 대형 소책자가 슬리브(Sleeve) 케이스로 특별 포장 된 초회한정반과 통상반으로 2가지 타입으로 발매하였다.\"\n",
      "[18] out: \"검정 CD 케이스와 대형 소책자가 슬리브(Sleeve) 케이스로 특별 포장된 초회한정반과 통상반으로 2가지 타입으로 발매하였다.\" (accuracy: 96.5%, sim: 92.3%=12/13)\n",
      "\n",
      "[19] in : \"첫 회 특전은 HAPPY 슬리브 패키지, HAPPY 포토 북 (IN NEW YORK), HAPPY 파노라마 스티커, HAPPY 스페셜 이벤트 참가 응모권, HAPPY 보너스 트랙 수록 이상 5가지.\"\n",
      "[19] out: \"첫회 특전은 HAPPY 슬리브 패키지, HAPPY 포토북(INNEW YORK), HAPPY 파노라마스티커, HAPPY 스페셜 이벤트 참가응모권, HAPPY 보너스 트랙 수록 이상 5가지.\" (accuracy: 92.9%, sim: 76.0%=19/25)\n",
      "\n",
      "[20] in : \"15곡째의 보너스 트랙은 지금까지 V6 싱글곡 메들리가 MEGAMIX로 수록.\"\n",
      "[20] out: \"15곡째의 보너스 트랙은 지금까지 V6 싱글곡 메들리가 MEGAMIX로 수록.\" (accuracy: 100.0%, sim: 100.0%=8/8)\n",
      "\n",
      "[21] in : \"《\"LUCKY\" 20th Century, Coming Century to be continued...》는 V6가 1999년 8월 18일에 발매한 4번째 정규 앨범이다.\"\n",
      "[21] out: \"《\"LUCKY \"20th Century, Coming Century tobecontinued...》는 V6가 1999년 8월 18일에 발매한 4번째 정규 앨범이다.\" (accuracy: 94.8%, sim: 80.0%=12/15)\n",
      "\n",
      "[22] in : \"초회한정반과 통상반으로 2가지 타입으로 발매. 초회한정반 특전은 쿠키향이 나도록 특수 제작 CD 케이스, V6 자켓 스티커.\"\n",
      "[22] out: \"초회한정반과 통상반으로 2가지 타입으로 발매. 초회한정반 특전은 쿠키 향이나도록 특수 제작 CD 케이스, V6 자켓 스티커.\" (accuracy: 96.2%, sim: 93.3%=14/15)\n",
      "\n",
      "[23] in : \"초회한정반 케이스가 특수 패키지로 제작된 건 첫 번째 앨범 《SINCE 1995 ~ FOREVER》 이후 3년 만이다.\"\n",
      "[23] out: \"초회한정반 케이스가 특수 패키지로 제작된 건 첫 번째 앨범 《SINCE 1995~FOREVER》 이후 3년 만이다.\" (accuracy: 96.0%, sim: 86.7%=13/15)\n",
      "\n",
      "[24] in : \"마지막 트랙에 리믹스가 들어가 있는 것도 3년 만이다.\"\n",
      "[24] out: \"마지막 트랙에 리믹스가 들어가 있는 것도 3년 만이다.\" (accuracy: 100.0%, sim: 100.0%=7/7)\n",
      "\n",
      "[25] in : \"《SINCE 1995 ~ FOREVER》에서는 수록곡을 리믹스하였고, 이번 앨범에서는 20th Century, Coming Century로 각각 리믹스. 20th Century 리믹스는 처음 (Coming Century 리믹스는 2번째).\"\n",
      "[25] out: \"《SINCE 1995~FOREVER》에서는 수록곡을 리믹스하였고, 이번 앨범에서는 20th Century, Coming Century로 각각 리믹스. 20th Century 리믹스는 처음(Coming Century 리믹스는 2번째).\" (accuracy: 97.3%, sim: 85.7%=18/21)\n",
      "\n",
      "[26] in : \"《\"O\"-正.反.合.》(\"O\"-정.반.합.)은 대한민국에서 발매한 동방신기의 세 번째 정규 음반이다.\"\n",
      "[26] out: \"《\"O\"-正.反.合.》(\"O\" -정. 반. 합.)은 대한민국에서 발매한 동방신기의 세 번째 정규 음반이다.\" (accuracy: 93.8%, sim: 100.0%=7/7)\n",
      "\n",
      "[27] in : \"CD\"\n",
      "[27] out: \"CD\" (accuracy: 100.0%, sim: 100.0%=0/0)\n",
      "\n",
      "[28] in : \"트랙 (A,B Ver./C,D Ver.)\"\n",
      "[28] out: \"트랙(A, BVer./C, DVer.)\" (accuracy: 72.2%, sim: 0.0%=0/3)\n",
      "\n",
      "[29] in : \"1/1\"\n",
      "[29] out: \"1/1\" (accuracy: 100.0%, sim: 100.0%=0/0)\n",
      "\n",
      "[30] in : \"2/2\"\n",
      "[30] out: \"2/2\" (accuracy: 100.0%, sim: 100.0%=0/0)\n",
      "\n",
      "[31] in : \"3/3\"\n",
      "[31] out: \"3/3\" (accuracy: 100.0%, sim: 100.0%=0/0)\n",
      "\n",
      "[32] in : \"4/\"\n",
      "[32] out: \"4/\" (accuracy: 100.0%, sim: 100.0%=0/0)\n",
      "\n",
      "[33] in : \"5/5\"\n",
      "[33] out: \"5/5\" (accuracy: 100.0%, sim: 100.0%=0/0)\n",
      "\n",
      "[34] in : \"6/7\"\n",
      "[34] out: \"6/7\" (accuracy: 100.0%, sim: 100.0%=0/0)\n",
      "\n",
      "[35] in : \"7/8\"\n",
      "[35] out: \"7/8\" (accuracy: 100.0%, sim: 100.0%=0/0)\n",
      "\n",
      "[36] in : \"8/9\"\n",
      "[36] out: \"8/9\" (accuracy: 100.0%, sim: 100.0%=0/0)\n",
      "\n",
      "[37] in : \"9/10\"\n",
      "[37] out: \"9/10\" (accuracy: 100.0%, sim: 100.0%=0/0)\n",
      "\n",
      "[38] in : \"10/11\"\n",
      "[38] out: \"10/11\" (accuracy: 100.0%, sim: 100.0%=0/0)\n",
      "\n",
      "[39] in : \"11/\"\n",
      "[39] out: \"11/\" (accuracy: 100.0%, sim: 100.0%=0/0)\n",
      "\n",
      "[40] in : \"/4\"\n",
      "[40] out: \"/4\" (accuracy: 100.0%, sim: 100.0%=0/0)\n",
      "\n",
      "[41] in : \"/6\"\n",
      "[41] out: \"/6\" (accuracy: 100.0%, sim: 100.0%=0/0)\n",
      "\n",
      "[42] in : \"/12\"\n",
      "[42] out: \"/12\" (accuracy: 100.0%, sim: 100.0%=0/0)\n",
      "\n",
      "[43] in : \"극장드라마 \"VACATION\" Highlight\"\n",
      "[43] out: \"극장 드라마\"VACATION\" Highlight\" (accuracy: 91.3%, sim: 50.0%=1/2)\n",
      "\n",
      "[44] in : \"극장드라마 \"VACATION\" M/V '그리고.... (Holding Back the Tears...)'\"\n",
      "[44] out: \"극장 드라마\"VACATION\"M/V'그리고.... (Holding Back the Tears...)'\" (accuracy: 92.0%, sim: 57.1%=4/7)\n",
      "\n",
      "[45] in : \"Jacket Sketch in Prague*\"\n",
      "[45] out: \"Jacket Sketchin Prague *\" (accuracy: 90.0%, sim: 66.7%=2/3)\n",
      "\n",
      "[46] in : \"Interview About TVXQ! Member's\"\n",
      "[46] out: \"Interview About TVXQ! Member's\" (accuracy: 100.0%, sim: 100.0%=3/3)\n",
      "\n",
      "[47] in : \"\"O\"- 正.反.合 M/V Making Film\"\n",
      "[47] out: \"\"O\"-正.反.合 M/VMaking Film\" (accuracy: 90.5%, sim: 50.0%=2/4)\n",
      "\n",
      "[48] in : \"\"O\"- 正.反.合 M/V (2:00 Ver.)\"\n",
      "[48] out: \"\"O\"-正.反.合 M/V (2:00 Ver.)\" (accuracy: 95.2%, sim: 75.0%=3/4)\n",
      "\n",
      "[49] in : \"東方神起's Exclusive Story : Something New!*\"\n",
      "[49] out: \"東方神起's Exclusive Story: Something New!*\" (accuracy: 97.1%, sim: 80.0%=4/5)\n",
      "\n",
      "[50] in : \"talk(1) 극장드라마 'Vacation' Introduction\"\n",
      "[50] out: \"talk (1) 극장 드라마'Vacation' Introduction\" (accuracy: 90.9%, sim: 66.7%=2/3)\n",
      "\n",
      "[51] in : \"극장드라마 'Vacation' (Full Ver.)\"\n",
      "[51] out: \"극장 드라마'Vacation'(Full Ver.)\" (accuracy: 87.5%, sim: 33.3%=1/3)\n",
      "\n",
      "[52] in : \"talk(2) 극장드라마 'Vacation' NG Special Behind\"\n",
      "[52] out: \"talk (2) 극장 드라마'Vacation'NG Special Behind\" (accuracy: 88.9%, sim: 60.0%=3/5)\n",
      "\n",
      "[53] in : \"극장드라마 'Vacation' NG Special\"\n",
      "[53] out: \"극장 드라마'Vacation'NG Special\" (accuracy: 87.0%, sim: 33.3%=1/3)\n",
      "\n",
      "[54] in : \"talk(1) The 3rd Album “O”-正.反.合. Showcase Introduction\"\n",
      "[54] out: \"talk (1) The 3rd Album “O”-正.反.合. Showcase Introduction\" (accuracy: 97.9%, sim: 100.0%=6/6)\n",
      "\n",
      "[55] in : \"The 3rd Album “O”-正.反.合. Showcase (No Cut ver.)\"\n",
      "[55] out: \"The 3rd Album “O”-正.反.合. Showcase (No Cutver.)\" (accuracy: 97.4%, sim: 85.7%=6/7)\n",
      "\n",
      "[56] in : \"talk(2) 'Hug' Memory\"\n",
      "[56] out: \"talk (2)'Hug'Memory\" (accuracy: 82.4%, sim: 0.0%=0/2)\n",
      "\n",
      "[57] in : \"MusicVideo 'Hug'\"\n",
      "[57] out: \"Music Video'Hug'\" (accuracy: 85.7%, sim: 0.0%=0/1)\n",
      "\n",
      "[58] in : \"talk(3) 'The Way U Are' Memory\"\n",
      "[58] out: \"talk (3)'The Way U Are'Memory\" (accuracy: 87.5%, sim: 60.0%=3/5)\n",
      "\n",
      "[59] in : \"MusicVideo 'The Way U Are'\"\n",
      "[59] out: \"Music Video'The Way U Are'\" (accuracy: 90.5%, sim: 75.0%=3/4)\n",
      "\n",
      "[60] in : \"talk(4) '믿어요 (I Believe)' Memory\"\n",
      "[60] out: \"talk (4)'믿어요 (I Believe)'Memory\" (accuracy: 88.9%, sim: 50.0%=2/4)\n",
      "\n",
      "[61] in : \"MusicVideo '믿어요 (I Believe)'\"\n",
      "[61] out: \"Music Video'믿어요 (I Believe)'\" (accuracy: 91.7%, sim: 66.7%=2/3)\n",
      "\n",
      "[62] in : \"talk(5) 'TRI-ANGLE' Memory\"\n",
      "[62] out: \"talk (5) 'TRI-ANGLE'Memory\" (accuracy: 91.3%, sim: 50.0%=1/2)\n",
      "\n",
      "[63] in : \"MusicVideo 'TRI-ANGLE'\"\n",
      "[63] out: \"Music Video'TRI-ANGLE'\" (accuracy: 90.0%, sim: 0.0%=0/1)\n",
      "\n",
      "[64] in : \"talk(6) 'Rising Sun' Memory\"\n",
      "[64] out: \"talk (6)'Rising Sun'Memory\" (accuracy: 87.0%, sim: 33.3%=1/3)\n",
      "\n",
      "[65] in : \"MusicVideo 'Rising Sun'\"\n",
      "[65] out: \"Music Video'Rising Sun'\" (accuracy: 90.0%, sim: 50.0%=1/2)\n",
      "\n",
      "[66] in : \"talk(7) “O”-正.反.合. Memory\"\n",
      "[66] out: \"talk (7) “O”-正.反.合. Memory\" (accuracy: 95.5%, sim: 100.0%=2/2)\n",
      "\n",
      "[67] in : \"MusicVideo “O”-正.反.合.\"\n",
      "[67] out: \"Music Video “O”-正.反.合.\" (accuracy: 94.7%, sim: 100.0%=1/1)\n",
      "\n",
      "[68] in : \"talk(8) '풍선(Balloons)' Memory\"\n",
      "[68] out: \"talk (8)'풍선(Balloons)'Memory\" (accuracy: 88.5%, sim: 0.0%=0/2)\n",
      "\n",
      "[69] in : \"MusicVideo '풍선(Balloons)'\"\n",
      "[69] out: \"Music Video'풍선(Balloons)'\" (accuracy: 91.3%, sim: 0.0%=0/1)\n",
      "\n",
      "[70] in : \"'풍선(Balloons)' M/V Making Film\"\n",
      "[70] out: \"'풍선(Balloons)'M/VMaking Film\" (accuracy: 92.3%, sim: 33.3%=1/3)\n",
      "\n",
      "[71] in : \"동방신기's Exclusive Story2 : Our Secret Code++\"\n",
      "[71] out: \"동방신기's Exclusive Story 2 : Our Secret Code++\" (accuracy: 97.2%, sim: 100.0%=6/6)\n",
      "\n",
      "[72] in : \"2006년 10월 15일, 리더 유노윤호가 KBS 별관서 진행된 여걸식스 녹화 후 편의점에서 음료수를 사러가던 중 팬을 가장한 한 여성으로부터 건네받은 본드 음료수를 마시고 구토증세를 보여 즉시 병원으로 옮겨졌다.\"\n",
      "[72] out: \"2006년 10월 15일, 리더 유노윤호가 KBS 별 관 서 진행된 여걸식스 녹화 후 편의점에서 음료수를 사러가던 중팬을 가장한한 여성으로부터 건네 받은 본드 음료수를 마시고 구토증세를 보여 즉시 병원으로 옮겨졌다.\" (accuracy: 94.5%, sim: 92.6%=25/27)\n",
      "\n",
      "[73] in : \"한 현장 관계자에 따르면 당시 음료를 마신 유노윤호는 곧바로 구토를 했으며, 당초 피를 토한 것으로 알려졌으나 출혈은 없었던 것으로 밝혀졌다.\"\n",
      "[73] out: \"한 현장 관계자에 따르면 당시 음료를 마신유노윤호는 곧바로 구토를 했으며, 당초피를 토한 것으로 알려졌으나 출혈은 없었던 것으로 밝혀졌다.\" (accuracy: 96.6%, sim: 89.5%=17/19)\n",
      "\n",
      "[74] in : \"유노윤호는 '여걸식스' 녹화를 중단하고 곧바로 병원으로 후송돼 진단을 받았다.\"\n",
      "[74] out: \"유노윤호는 '여걸식스'녹화를 중단하고 곧바로 병원으로 후송돼진단을 받았다.\" (accuracy: 94.1%, sim: 75.0%=6/8)\n",
      "\n",
      "[75] in : \"안티팬의 쪽지에는 '애들이나 좋아하는데 인기가 많은 줄 안다.\"\n",
      "[75] out: \"안티팬의 쪽지에는 '애들이나 좋아하는 데 인기가 많은 줄안다.\" (accuracy: 92.3%, sim: 85.7%=6/7)\n",
      "\n",
      "[76] in : \"춤을 잘 추는 줄로 착각한다.\"\n",
      "[76] out: \"춤을 잘추는 줄로 착각한다.\" (accuracy: 90.9%, sim: 75.0%=3/4)\n",
      "\n",
      "[77] in : \"죽여버리고 싶다.\"\n",
      "[77] out: \"죽여버리고 싶다.\" (accuracy: 100.0%, sim: 100.0%=1/1)\n",
      "\n",
      "[78] in : \"'는 등의 내용이 포함되어 있었다.\"\n",
      "[78] out: \"'는 등의 내용이 포함되어 있었다.\" (accuracy: 100.0%, sim: 100.0%=4/4)\n",
      "\n",
      "[79] in : \"\"생태 및 사회 보호를 위한\" 전우크라이나당()은 2003년 11월 등록한 우크라이나의 정당이다.\"\n",
      "[79] out: \"\"생태 및 사회보호를 위한 \"전우 크라이나당()은 2003년 11월 등록한 우크라이나의 정당이다.\" (accuracy: 90.7%, sim: 80.0%=8/10)\n",
      "\n",
      "[80] in : \"이 당은 2007년 까지는 전국적인 범위의 선거에 참여하지 않았다.\"\n",
      "[80] out: \"이당은 2007년까지는 전국적인 범위의 선거에 참여하지 않았다.\" (accuracy: 92.9%, sim: 75.0%=6/8)\n",
      "\n",
      "[81] in : \"2007년 우크라이나 총선에서 이 당은 크리스찬 블록의 일부로 참여했으나 의석을 얻는 데 실패했다.\"\n",
      "[81] out: \"2007년 우크라이나 총선에서 이당은 크리스찬 블록의 일부로 참여했으나 의석을 얻는 데 실패했다.\" (accuracy: 97.6%, sim: 91.7%=11/12)\n",
      "\n",
      "[82] in : \"2012년 우크라이나 총선에서는 참여하지 않았다.\"\n",
      "[82] out: \"2012년 우크라이나 총선에서는 참여하지 않았다.\" (accuracy: 100.0%, sim: 100.0%=4/4)\n",
      "\n",
      "[83] in : \"2014년 우크라이나 총선에도 참여하지 않았다.\"\n",
      "[83] out: \"2014년 우크라이나 총선에도 참여하지 않았다.\" (accuracy: 100.0%, sim: 100.0%=4/4)\n",
      "\n",
      "[84] in : \"$는 통화기호 가운데 하나로, 달러 기호 또는 페소 기호라고 부른다.\"\n",
      "[84] out: \"$는 통화 기호가운데 하나로, 달러기호 또는 페소기호라고 부른다.\" (accuracy: 85.7%, sim: 66.7%=6/9)\n",
      "\n",
      "[85] in : \"《[&] Seotaiji 15th Anniversary》는 서태지 데뷔 15주년을 기념하여 서태지와 아이들 시절 1집부터 솔로 7집까지의 앨범이 수록되어 있는 박스 세트이다.\"\n",
      "[85] out: \"《[&] Seotaiji 15th Anniversary》는 서태지 데뷔 15주년을 기념하여 서태지와 아이들 시절 1집부터 솔로 7집까지의 앨범이 수록되어 있는 박스세트이다.\" (accuracy: 98.7%, sim: 94.4%=17/18)\n",
      "\n",
      "[86] in : \"DVD에는 서태지와 아이들 이전에 몸 담았던 메탈밴드 시나위 시절의 영상, 각종 뮤직비디오, 미공개 영상, 광고 영상 등 데뷔때 부터 2007년 당시까지의 자료들을 모아놓았다.\"\n",
      "[86] out: \"DVD에는 서태지와 아이들이전에 몸담았던 메탈 밴드 시나 위 시절의 영상, 각종 뮤직비디오, 미공개 영상, 광고 영상 등 데뷔 때부터 2007년 당시까지의 자료들을 모아 놓았다.\" (accuracy: 90.7%, sim: 86.4%=19/22)\n",
      "\n",
      "[87] in : \"음반에는 1집부터 7집까지 서태지의 모든 정규 앨범의 전곡이 수록되어 있으며, 《‘07 교실 이데아 (Remix)》,《‘07 COME BACK HOME (Remix)》, 《‘03 대경성 (Remix)》,《‘03 인터넷 전쟁 (Remix)》를 첫 공개 했다.\"\n",
      "[87] out: \"음반에는 1집부터 7집까지 서태지의 모든 정규 앨범의 전곡이 수록되어 있으며, 《‘07 교실이데아(Remix)》, 《‘07 COMEBACK HOME(Remix)》, 《‘03대 경성(Remix)》, 《‘03 인터넷 전쟁(Remix)》를 첫 공개했다.\" (accuracy: 90.5%, sim: 69.2%=18/26)\n",
      "\n",
      "[88] in : \"두 장의 DVD에는 서태지가 발표한 모든 뮤직비디오와 공연 실황, 미공개 영상, 미공개 뮤직비디오 《난 알아요》가 포함되어 있다.\"\n",
      "[88] out: \"두 장의 DVD에는 서태지가 발표한 모든 뮤직비디오와 공연 실황, 미공개 영상, 미공개 뮤직비디오 《난 알아요》가 포함되어 있다.\" (accuracy: 100.0%, sim: 100.0%=16/16)\n",
      "\n",
      "[89] in : \"디지털 싱글로만 판매된《' Watch Out》이 수록되었고 방송활동에 사용된 《 하여가 (Remix Hip-Hop Version)》, 《 난 알아요 (TV Edit)》,《 하여가 (TV Edit)'》등이 수록되었다.\"\n",
      "[89] out: \"디지털 싱글로만 판매된 《' Watch Out》이 수록되었고 방송 활동에 사용된 《하여가(Remix Hip-Hop Version)》, 《난 알아요(TVEdit)》, 《하여가(TVEdit)'》 등이 수록되었다.\" (accuracy: 87.8%, sim: 61.9%=13/21)\n",
      "\n",
      "[90] in : \"특히 모든 음원을 리마스터링 작업을 거쳐 음질을 높임과 동시에 아날로그 영상들은 디지털화 하면서 화질을 높였다.\"\n",
      "[90] out: \"특히 모든 음원을 리마스터링 작업을 거쳐 음질을 높임과 동시에 아날로그 영상들은 디지털화하면서 화질을 높였다.\" (accuracy: 97.9%, sim: 92.9%=13/14)\n",
      "\n",
      "[91] in : \"특히 한정판으로 1만 5000장만 판매되어 금세 전량 매진되었으며 가격은 인터넷 경매에서 100만원 이상 호가하기도 했다.\"\n",
      "[91] out: \"특히 한정판으로 1만 5000장만 판매되어 금세 전량 매진되었으며 가격은 인터넷 경매에서 100만원 이 상호가하기도 했다.\" (accuracy: 96.2%, sim: 92.9%=13/14)\n",
      "\n",
      "[92] in : \"1만5000장 한정판이 모두 품절되자 추가 발매요구 및 일반판 발매논란이 있었다.\"\n",
      "[92] out: \"1만 5000장 한정판이 모두 품절되자 추가 발매 요구 및 일반 판 발매 논란이 있었다.\" (accuracy: 88.6%, sim: 100.0%=9/9)\n",
      "\n",
      "[93] in : \"박스세트의 추가발매는 없었으나 2009년에 1집에서 7집까지 정규앨범이 각각 재발매 되었고, 구성은 박스세트에서의 앨범들과 마찬가지로 원곡의 마스터링 혹은 리레코딩 버전과 라이브, 리믹스 등의 히든트랙으로 동일하다.\"\n",
      "[93] out: \"박스세트의 추가 발매는 없었으나 2009년에 1집에서 7집까지 정규 앨범이 각각 재 발매되었고, 구성은 박스세트에서의 앨범들과 마찬가지로 원곡의 마스터링 혹은 리 레코딩 버전과 라이브, 리믹스 등의 히든 트랙으로 동일하다.\" (accuracy: 93.8%, sim: 95.7%=22/23)\n",
      "\n",
      "[94] in : \"서태지가 직접 활동한 것은 전무하며 음반 발매 이후 삼성전자에서 서태지 기념앨범 음원 14곡과 뮤직비디오, 미공개 동영상 등 스페셜 에디션에 내장된 MP3 ‘옙 P2 서태지 스페셜 에디션’을 판매했다.\"\n",
      "[94] out: \"서태지가 직접 활동한 것은 전무하며 음반 발매 이후 삼성전자에서 서태지 기념 앨범 음원 14곡과 뮤직비디오, 미공개 동영상 등 스페셜 에디션에 내장된 MP3 ‘옙P2 서태지스페셜 에디션’을 판매했다.\" (accuracy: 96.5%, sim: 92.3%=24/26)\n",
      "\n",
      "[95] in : \"이후 2007년 12월 1일에는 서태지의 데뷔 15주년을 기념하는 콘서트가 열려 45RPM, 나윤권, 에픽하이, 피아, 스윗소로우 등이 서태지의 노래를 불렀으며, 빅뱅은 〈필승〉으로 게릴라 콘서트를 했다.\"\n",
      "[95] out: \"이후 2007년 12월 1일에는 서태지의 데뷔 15주년을 기념하는 콘서트가 열려 45 RPM, 나윤권, 에픽하이, 피아, 스윗소로우 등이 서태지의 노래를 불렀으며, 빅뱅은 〈필승〉으로 게릴라 콘서트를 했다.\" (accuracy: 98.9%, sim: 100.0%=23/23)\n",
      "\n",
      "[96] in : \"모든 공연 순서가 끝난 뒤 서태지의 최근 모습이 담긴 영상을 공개하고 팬들을 향한 편지를 보내기도 했다.\"\n",
      "[96] out: \"모든 공연 순서가 끝난 뒤 서태지의 최근 모습이 담긴 영상을 공개하고 팬들을 향한 편지를 보내기도 했다.\" (accuracy: 100.0%, sim: 100.0%=15/15)\n",
      "\n",
      "[97] in : \"이 공연은 12월 25일에 엠넷을 통해 방송했으며, 12월 31일에는 포털사이트 네이버를 통해 15주년 기념 온라인 전시관 을 열었다.\"\n",
      "[97] out: \"이 공연은 12월 25일에 엠넷을 통해 방송했으며, 12월 31일에는 포털 사이트 네이버를 통해 15주년 기념 온라인전시관을 열었다.\" (accuracy: 94.7%, sim: 88.2%=15/17)\n",
      "\n",
      "[98] in : \"서태지(서태지컴퍼니) - 프로듀서, 이그제큐티브 프로듀서, '07 리믹스\"\n",
      "[98] out: \"서태지(서태지컴퍼니) - 프로듀서, 이그제큐티브 프로듀서, '07 리믹스\" (accuracy: 100.0%, sim: 100.0%=6/6)\n",
      "\n",
      "[99] in : \"김석중 - '07 리믹스\"\n",
      "[99] out: \"김석중 -'07 리믹스\" (accuracy: 88.9%, sim: 66.7%=2/3)\n",
      "chek result OK.\n",
      "mean(accuracy): 94.60%, mean(sim): 79.59%\n",
      "secs/sentence: 0.0564\n",
      "average [00:00:05 0.6350] check\n",
      "\n"
     ]
    }
   ],
   "source": [
    "watch = WatchUtil()\n",
    "watch.start('check')\n",
    "log.info('chek result...')\n",
    "sentences = ['아버지가 방에 들어 가신다.', '가는 말이 고와야 오는 말이 곱다.']\n",
    "max_test_sentences = 100\n",
    "with gzip.open(sentences_file, 'rt') as f:\n",
    "    for line in f:\n",
    "        if len(sentences) >= max_test_sentences:\n",
    "            break\n",
    "        sentences.append(line.strip())\n",
    "log.info('len(sentences): %s' % NumUtil.comma_str(len(sentences)))\n",
    "\n",
    "accuracies, sims = [], []\n",
    "with tf.Session() as sess:\n",
    "    graph = WordSpacing.build_FFNN(n_features, n_classes, n_hidden1, learning_rate, layers=layers)\n",
    "    X, Y, predicted, accuracy = graph['X'], graph['Y'], graph['predicted'], graph['accuracy']\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    try:\n",
    "        restored = saver.restore(sess, model_file)\n",
    "    except:\n",
    "        log.error('restore failed. model_file: %s' % model_file)\n",
    "    try:\n",
    "        for i, s in enumerate(sentences):\n",
    "            log.info('')\n",
    "            log.info('[%s] in : \"%s\"' % (i, s))\n",
    "            features, labels = WordSpacing.sentence2features_labels(s, left_gram, right_gram)\n",
    "            dataset = DataSet(features=features, labels=labels, features_vector=features_vector, labels_vector=labels_vector)\n",
    "            dataset.convert_to_one_hot_vector()\n",
    "            if len(dataset) > 0:\n",
    "                _predicted, _accuracy = sess.run([predicted, accuracy], feed_dict={X: dataset.features, Y: dataset.labels})  # Accuracy report\n",
    "\n",
    "                generated_sentence = WordSpacing.spacing(s.replace(' ', ''), _predicted)\n",
    "                sim, correct, total = WordSpacing.sim_two_sentence(s, generated_sentence, left_gram=left_gram, right_gram=right_gram)\n",
    "\n",
    "                accuracies.append(_accuracy)\n",
    "                sims.append(sim)\n",
    "\n",
    "                log.info('[%s] out: \"%s\" (accuracy: %.1f%%, sim: %.1f%%=%s/%s)' % (i, generated_sentence, _accuracy * 100, sim * 100, correct, total))\n",
    "    except:\n",
    "        log.error(traceback.format_exc())\n",
    "\n",
    "log.info('chek result OK.')\n",
    "log.info('mean(accuracy): %.2f%%, mean(sim): %.2f%%' % (np.mean(accuracies)*100, np.mean(sims)*100))\n",
    "log.info('secs/sentence: %.4f' % (watch.elapsed('check') / len(sentences)))\n",
    "log.info(watch.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tip. 입력 데이터 증가, 파라미터 조절, 레이어 증가... 성능을 높이려면, 뭐 부터 해야 할까요?\n",
    "- 여러 가지 해보려면 시간이 적게 걸리는 것부터 해야 겠죠.\n",
    "- 파라미터 조절 or 레이어 증가 -> 입력 데이터 증가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
